{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"mask_detector.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"972dcbd5"},"source":["This project will realize a real-time system to detect if the person showing up<br> on the webcam is wearing a mask or not, the fack mask detector model\n","will be<br> trained using Keras and OpenCV. The whole project will be realized with two parts:<br>\n","1. Using Keras in a python script to train the face detector model.\n","2. Using OpenCV to test results in a real-time webcam\n","\n"],"id":"972dcbd5"},{"cell_type":"code","metadata":{"id":"bdc4a66a"},"source":["\"\"\"\n","1. Import essential libraries and modules to be used\n","\"\"\"\n","import cv2\n","import imutils\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import RMSprop\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization,\\\n","Activation, MaxPooling2D, Flatten, Dense, Dropout\n","from keras.models import Model, load_model\n","from keras.callbacks import TensorBoard, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","from sklearn.utils import shuffle\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"],"id":"bdc4a66a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c2875915"},"source":["\"\"\"\n","2. Build the neural network\n","This network consists of two pairs od Conv and MaxPooling layers to extract features\n","from the image dataset, which is followed by two layers Flatten and Dropout to convert\n","extracted feature data into 1D vector and prevent overfitting from happening\n","\"\"\"\n","model = Sequential([\n","    # The 1st pair of Conv and MaxPool layers, the input shape is (300, 300, 3), which is the size of\n","    # the reshaped true color image from the image dataset.\n","    Conv2D(100, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n","    MaxPooling2D(2, 2),\n","    \n","    # The 2nd pair of Conv and MaxPool layers\n","    Conv2D(100, (3, 3), activation='relu'),\n","    MaxPooling2D(2, 2),\n","    \n","    Flatten(),\n","    Dropout(0.5),\n","    # Fully-connected layers\n","    Dense(50, activation='relu'),\n","    Dense(2, activation='softmax')    \n","])\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['acc'])"],"id":"c2875915","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00be12cf","executionInfo":{"status":"ok","timestamp":1638812106714,"user_tz":-120,"elapsed":265,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"35c7c744-b2fc-4b46-b6ed-13a5e7a24ee7"},"source":["\"\"\"\n","3. Image Data Generation/Augmentation\n","This part would better be run only once,otherwise this will happen:\n","error OSError: broken data stream when reading image file\n","\"\"\"\n","TRAINING_DIR = \"./train\"\n","# Perform various data augmentation operations (e.g.,rotation, rescaling etc.)\n","# to extend the scale of training samples in the dataset\n","train_datagen = ImageDataGenerator(rescale=1.0/255,\n","                  rotation_range=40,\n","                  width_shift_range=0.2,\n","                  height_shift_range=0.2,\n","                  shear_range=0.2,\n","                  zoom_range=0.2,\n","                  horizontal_flip=True,\n","                  fill_mode='nearest')\n","\n","train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n","                           batch_size=10,\n","                           target_size=(300, 300))\n","\n","VALIDATION_DIR = \"./test\"\n","validation_datagen = ImageDataGenerator(rescale=1.0/255)\n","\n","validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n","                                batch_size=10,\n","                                target_size=(300, 300))"],"id":"00be12cf","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3273 images belonging to 2 classes.\n","Found 819 images belonging to 2 classes.\n"]}]},{"cell_type":"code","metadata":{"id":"cca801dc"},"source":["\"\"\"\n","4. A callback checkpoint is initialized to keep saving the best model after each epoch\n","while training\n","\"\"\"\n","check_point = ModelCheckpoint('model2-{epoch:03d}.h5', monitor='val_loss', verbose=0,\n","                             save_best_only=True, mode='auto')"],"id":"cca801dc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b0463dd","executionInfo":{"status":"ok","timestamp":1638814239116,"user_tz":-120,"elapsed":2101726,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"b0b2f12f-9ffb-4494-9b1d-b77cee5ed8fb"},"source":["\"\"\"\n","5. Train the model\n","\"\"\"\n","history = model.fit(train_generator,\n","           epochs = 20,\n","           validation_data = validation_generator,\n","           callbacks=[check_point])"],"id":"5b0463dd","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","183/328 [===============>..............] - ETA: 42s - loss: 0.4465 - acc: 0.8044"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  \"Palette images with Transparency expressed in bytes should be \"\n"]},{"output_type":"stream","name":"stdout","text":["328/328 [==============================] - 115s 319ms/step - loss: 0.3995 - acc: 0.8307 - val_loss: 0.4214 - val_acc: 0.8242\n","Epoch 2/20\n","328/328 [==============================] - 103s 313ms/step - loss: 0.3161 - acc: 0.8784 - val_loss: 0.4784 - val_acc: 0.7924\n","Epoch 3/20\n","328/328 [==============================] - 103s 313ms/step - loss: 0.3057 - acc: 0.8769 - val_loss: 0.4595 - val_acc: 0.8107\n","Epoch 4/20\n","328/328 [==============================] - 103s 313ms/step - loss: 0.2710 - acc: 0.8937 - val_loss: 0.3487 - val_acc: 0.8632\n","Epoch 5/20\n","328/328 [==============================] - 102s 311ms/step - loss: 0.2601 - acc: 0.8928 - val_loss: 0.4612 - val_acc: 0.8230\n","Epoch 6/20\n","328/328 [==============================] - 102s 312ms/step - loss: 0.2326 - acc: 0.9102 - val_loss: 0.3347 - val_acc: 0.8730\n","Epoch 7/20\n","328/328 [==============================] - 102s 311ms/step - loss: 0.2415 - acc: 0.9004 - val_loss: 0.3570 - val_acc: 0.8755\n","Epoch 8/20\n","328/328 [==============================] - 103s 313ms/step - loss: 0.2170 - acc: 0.9160 - val_loss: 0.4167 - val_acc: 0.8315\n","Epoch 9/20\n","328/328 [==============================] - 102s 312ms/step - loss: 0.2342 - acc: 0.9129 - val_loss: 0.3471 - val_acc: 0.8584\n","Epoch 10/20\n","328/328 [==============================] - 102s 310ms/step - loss: 0.2080 - acc: 0.9169 - val_loss: 0.3695 - val_acc: 0.8474\n","Epoch 11/20\n","328/328 [==============================] - 102s 312ms/step - loss: 0.2039 - acc: 0.9221 - val_loss: 0.3565 - val_acc: 0.8584\n","Epoch 12/20\n","328/328 [==============================] - 103s 313ms/step - loss: 0.1931 - acc: 0.9258 - val_loss: 0.3164 - val_acc: 0.8657\n","Epoch 13/20\n","328/328 [==============================] - 102s 312ms/step - loss: 0.2079 - acc: 0.9215 - val_loss: 0.3784 - val_acc: 0.8584\n","Epoch 14/20\n","328/328 [==============================] - 102s 312ms/step - loss: 0.1843 - acc: 0.9236 - val_loss: 0.2978 - val_acc: 0.8852\n","Epoch 15/20\n","328/328 [==============================] - 102s 312ms/step - loss: 0.1783 - acc: 0.9291 - val_loss: 0.3323 - val_acc: 0.8620\n","Epoch 16/20\n","328/328 [==============================] - 102s 310ms/step - loss: 0.1795 - acc: 0.9276 - val_loss: 0.3242 - val_acc: 0.8694\n","Epoch 17/20\n","328/328 [==============================] - 103s 312ms/step - loss: 0.1726 - acc: 0.9328 - val_loss: 0.3532 - val_acc: 0.8596\n","Epoch 18/20\n","328/328 [==============================] - 103s 315ms/step - loss: 0.1744 - acc: 0.9303 - val_loss: 0.2977 - val_acc: 0.8755\n","Epoch 19/20\n","328/328 [==============================] - 104s 315ms/step - loss: 0.1659 - acc: 0.9374 - val_loss: 0.2771 - val_acc: 0.8877\n","Epoch 20/20\n","328/328 [==============================] - 102s 310ms/step - loss: 0.1571 - acc: 0.9374 - val_loss: 0.3855 - val_acc: 0.8523\n"]}]},{"cell_type":"code","metadata":{"id":"8acd3b5e"},"source":["import zipfile\n","import os\n","files = zipfile.ZipFile('dataset.zip', 'r')\n","files.extractall(os.getcwd())"],"id":"8acd3b5e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Me4l8Ak5MQXu"},"source":[""],"id":"Me4l8Ak5MQXu","execution_count":null,"outputs":[]}]}