{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training_and_evaluating_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP823X43ekTsNdzpHNK/Fih"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# mount the google drive on the colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"SJEW51CdCjk0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643369075269,"user_tz":-120,"elapsed":26413,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"7092c7f3-063e-46f4-c19b-9775b0df4344"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# move the datasets for more convenient usage\n","!cp /content/drive/MyDrive/Test_workspace/datasets.zip /content/"],"metadata":{"id":"cSxb34EOg4Yk","executionInfo":{"status":"ok","timestamp":1643369104274,"user_tz":-120,"elapsed":24158,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# uncompressed the zip file\n","import zipfile\n","import os\n","files = zipfile.ZipFile('datasets.zip', 'r')\n","files.extractall(os.getcwd())"],"metadata":{"id":"dwnN34nXhzNr","executionInfo":{"status":"ok","timestamp":1643369203068,"user_tz":-120,"elapsed":59231,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# build the class Config used for storing paths\n","import os\n","\n","class Config():\n","  # the path for the new directory \n","  BASE_PATH = \"/content/datasets/idc\"\n","  # the paths for the training,validation,and testing directories using the\n","  # bat\n","  TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n","  VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n","  TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])"],"metadata":{"id":"wwkKVl10PeyN","executionInfo":{"status":"ok","timestamp":1643369208525,"user_tz":-120,"elapsed":659,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5ElJFGbjZyUN","executionInfo":{"status":"ok","timestamp":1643369216965,"user_tz":-120,"elapsed":3181,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}}},"outputs":[],"source":["\"\"\"\n","The model to be built is a CNN named as CancerNet. This network performs the operations below:\n","1.Use 3x3 filters\n","2.Stack these filters on the top of others\n","3.Perform max-pooling\n","4.Use depthwise separable convolution (more efficient than the common convolution, but takes up less memory)\n","\"\"\"\n","\n","# import required libraries\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import BatchNormalization, \\\n","SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","\n","# declare a class with static methods to create the network\n","class CancerNet:\n","  @staticmethod\n","  def build(width, height, depth, class_num):\n","    # the variable used for building models\n","    model = Sequential()\n","    # the input size of the network\n","    shape = (height, width, depth)\n","    # the location of channels, after the convolution, the batch\n","    # normalization will happen on channels. The default location\n","    # of channel is at the last element of the input size \n","    # (channel_last), e.g., (batch, height, width, channel)\n","    channel_dim = -1\n","    \n","    # if the Conv2D layer's data format is channel_first, switch\n","    # the channel_dim's location (1)\n","    if K.image_data_format() == \"channels_first\":\n","      shape = (depth, height, width)\n","      channel_dim = 1\n","\n","    # Convolutional layers. It is combined with the higher stacking\n","    # of the architecture: depthwise conv -> relu -> pool layers\n","    # and a great number of filters. \n","    model.add(SeparableConv2D(32, (3, 3), padding=\"same\", input_shape=shape))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=channel_dim))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=channel_dim))\n","    model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=channel_dim))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=channel_dim))\n","    model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=channel_dim))\n","    model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization(axis=channel_dim))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    # convert the multi-dimensional data to the one-dimensional. \n","    # it is often used for the transiton between Conv layers\n","    # and Dense layers(Fully-connected layers)\n","    model.add(Flatten())\n","    model.add(Dense(256))\n","    model.add(Activation(\"relu\"))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","\n","    # Prediction probabilities only has two types: \n","    # the probability for category 0 or 1\n","    model.add(Dense(class_num))\n","    model.add(Activation(\"softmax\"))\n","\n","    return model"]},{"cell_type":"code","source":["\"\"\"\n","This part prepare the data to train and evaluate the model\n","\"\"\"\n","# the figure will not be shown after program running\n","import matplotlib\n","#matplotlib.use(\"Agg\")\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.optimizers import Adagrad\n","from tensorflow.python.keras.utils import np_utils\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","# lacking of the program file config's path\n","from imutils import paths \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","EPOCHS_NUM = 40  # the number of training epochs\n","INIT_LR = 1e-2  # the initialized learning rate\n","BATCH_SIZE = 32  # batch size\n","\n","# the sys paths of all training samples \n","train_paths = list(paths.list_images(Config.TRAIN_PATH))\n","# the number of samples in the training set\n","train_len = len(train_paths)\n","# the number of samples in the validation set\n","valid_len = len(list(paths.list_images(Config.VAL_PATH)))\n","# the number of samples in the testing dataset\n","test_len = len(list(paths.list_images(Config.TEST_PATH)))\n","\n","# get the labels for all training samples (from folders named 0 or 1),\n","# put them all in a list \n","train_labels = [int(path.split(os.path.sep)[-2]) for path in train_paths]\n","# convert the train labels' single digit to the one hot encoding\n","train_labels = np_utils.to_categorical(train_labels)\n","# add train_labels by rows to count the number of samples for each category\n","class_totals = train_labels.sum(axis=0)\n","# calculate the class_weights for the imbalanced classification\n","# e.g. the class_totals is [15, 13], the class_weight is\n","# [15/15, 15/13], this weights show that the category with less samples\n","# generally will gain more weights. Another calculation way [1/15, 1/13]\n","# reference link: https://keras.io/examples/structured_data/imbalanced_classification/\n","# how to resolve imbalanced issues: https://blog.csdn.net/weixin_39668479/article/details/112395317\n","class_weight = class_totals.max()/class_totals\n","\n","# perform the different data augmentation operations to the training set\n","train_aug = ImageDataGenerator(\n","    rescale = 1/255.0,\n","    rotation_range = 20,\n","    zoom_range = 0.05,\n","    width_shift_range = 0.1,\n","    height_shift_range = 0.1,\n","    shear_range = 0.05,   # shear exchange, keep x or y coordinate unchanged and change the coordinate on another axis\n","    horizontal_flip = True,\n","    vertical_flip = True,\n","    fill_mode = \"nearest\")\n","\n","# perform the data augmentation operation to the validation set\n","val_aug = ImageDataGenerator(rescale = 1/255.0)\n","\n","# perform the data augmentation operation to the test set\n","test_aug = ImageDataGenerator(rescale = 1/255.0)\n","\n","# generate batches of tensor image data with real-time data augmentation\n","train_gen = train_aug.flow_from_directory(\n","    Config.TRAIN_PATH,\n","    class_mode=\"categorical\",\n","    target_size=(48,48),  # the original size of each image is 50 x 50\n","    color_mode=\"rgb\",\n","    shuffle=True,\n","    batch_size=BATCH_SIZE)\n","\n","# generate batches of tensor image data with real-time data augmentation\n","val_gen = train_aug.flow_from_directory(\n","    Config.VAL_PATH,\n","    class_mode=\"categorical\",\n","    target_size=(48,48),  # the original size of each image is 50 x 50\n","    color_mode=\"rgb\",\n","    shuffle=False,\n","    batch_size=BATCH_SIZE)\n","\n","# generate batches of tensor image data with real-time data augmentation\n","test_gen = test_aug.flow_from_directory(\n","    Config.TEST_PATH,\n","    class_mode=\"categorical\",\n","    target_size=(48,48),  # the original size of each image is 50 x 50\n","    color_mode=\"rgb\",\n","    shuffle=False,\n","    batch_size=BATCH_SIZE)"],"metadata":{"id":"ABrDdmMRxKIX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643369233787,"user_tz":-120,"elapsed":8074,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"63eca817-0704-4016-e00f-47669d0fbcfa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 199818 images belonging to 2 classes.\n","Found 22201 images belonging to 2 classes.\n","Found 55505 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Model training with the adaptive gradients optimizer\n","\"\"\"\n","# Build the network with some parameters as parameters like\n","# the input size of network and number of categories\n","model = CancerNet.build(width=48, height=48, depth=3, class_num=2)\n","# Build the optimizer of adaptive gradients with the initial learning rate\n","# and the decay of the learning rate\n","opt = Adagrad(learning_rate=INIT_LR, decay=INIT_LR/EPOCHS_NUM)\n","# Compile the whole model\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","\n","# reference link:https://keras.io/examples/structured_data/imbalanced_classification/\n","class_weight={0: class_weight[0], 1: class_weight[1]}\n","# Training the model with the function fit_generator\n","training_records = model.fit(\n","          train_gen,\n","          steps_per_epoch = train_len//BATCH_SIZE,\n","          validation_data = val_gen,\n","          validation_steps = valid_len//BATCH_SIZE,\n","          class_weight = class_weight,\n","          epochs = EPOCHS_NUM)"],"metadata":{"id":"5uvaUsQL-iP8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643312302541,"user_tz":-120,"elapsed":12286539,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"7711dc61-879a-4ef5-bb9c-6cec0e8786b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","6244/6244 [==============================] - 303s 47ms/step - loss: 0.6032 - accuracy: 0.8169 - val_loss: 0.5352 - val_accuracy: 0.7657\n","Epoch 2/40\n","6244/6244 [==============================] - 290s 46ms/step - loss: 0.5565 - accuracy: 0.8303 - val_loss: 0.5873 - val_accuracy: 0.7458\n","Epoch 3/40\n","6244/6244 [==============================] - 289s 46ms/step - loss: 0.5481 - accuracy: 0.8334 - val_loss: 0.5856 - val_accuracy: 0.7376\n","Epoch 4/40\n","6244/6244 [==============================] - 304s 49ms/step - loss: 0.5419 - accuracy: 0.8341 - val_loss: 0.5681 - val_accuracy: 0.7513\n","Epoch 5/40\n","6244/6244 [==============================] - 306s 49ms/step - loss: 0.5398 - accuracy: 0.8355 - val_loss: 0.5787 - val_accuracy: 0.7452\n","Epoch 6/40\n","6244/6244 [==============================] - 292s 47ms/step - loss: 0.5384 - accuracy: 0.8375 - val_loss: 0.5673 - val_accuracy: 0.7509\n","Epoch 7/40\n","6244/6244 [==============================] - 290s 46ms/step - loss: 0.5387 - accuracy: 0.8372 - val_loss: 0.5344 - val_accuracy: 0.7673\n","Epoch 8/40\n","6244/6244 [==============================] - 308s 49ms/step - loss: 0.5364 - accuracy: 0.8369 - val_loss: 0.5525 - val_accuracy: 0.7564\n","Epoch 9/40\n","6244/6244 [==============================] - 290s 46ms/step - loss: 0.5339 - accuracy: 0.8381 - val_loss: 0.5543 - val_accuracy: 0.7588\n","Epoch 10/40\n","6244/6244 [==============================] - 310s 50ms/step - loss: 0.5338 - accuracy: 0.8381 - val_loss: 0.5603 - val_accuracy: 0.7555\n","Epoch 11/40\n","6244/6244 [==============================] - 292s 47ms/step - loss: 0.5353 - accuracy: 0.8385 - val_loss: 0.5694 - val_accuracy: 0.7505\n","Epoch 12/40\n","6244/6244 [==============================] - 292s 47ms/step - loss: 0.5344 - accuracy: 0.8382 - val_loss: 0.5427 - val_accuracy: 0.7609\n","Epoch 13/40\n","6244/6244 [==============================] - 292s 47ms/step - loss: 0.5336 - accuracy: 0.8381 - val_loss: 0.5643 - val_accuracy: 0.7494\n","Epoch 14/40\n","6244/6244 [==============================] - 305s 49ms/step - loss: 0.5335 - accuracy: 0.8378 - val_loss: 0.5644 - val_accuracy: 0.7492\n","Epoch 15/40\n","6244/6244 [==============================] - 287s 46ms/step - loss: 0.5346 - accuracy: 0.8373 - val_loss: 0.5691 - val_accuracy: 0.7466\n","Epoch 16/40\n","6244/6244 [==============================] - 303s 49ms/step - loss: 0.5326 - accuracy: 0.8379 - val_loss: 0.5425 - val_accuracy: 0.7600\n","Epoch 17/40\n","6244/6244 [==============================] - 301s 48ms/step - loss: 0.5314 - accuracy: 0.8388 - val_loss: 0.5573 - val_accuracy: 0.7536\n","Epoch 18/40\n","6244/6244 [==============================] - 286s 46ms/step - loss: 0.5335 - accuracy: 0.8379 - val_loss: 0.5522 - val_accuracy: 0.7564\n","Epoch 19/40\n","6244/6244 [==============================] - 283s 45ms/step - loss: 0.5318 - accuracy: 0.8387 - val_loss: 0.5660 - val_accuracy: 0.7497\n","Epoch 20/40\n","6244/6244 [==============================] - 301s 48ms/step - loss: 0.5321 - accuracy: 0.8393 - val_loss: 0.5451 - val_accuracy: 0.7601\n","Epoch 21/40\n","6244/6244 [==============================] - 287s 46ms/step - loss: 0.5312 - accuracy: 0.8384 - val_loss: 0.5545 - val_accuracy: 0.7562\n","Epoch 22/40\n","6244/6244 [==============================] - 304s 49ms/step - loss: 0.5324 - accuracy: 0.8393 - val_loss: 0.5562 - val_accuracy: 0.7539\n","Epoch 23/40\n","6244/6244 [==============================] - 296s 47ms/step - loss: 0.5289 - accuracy: 0.8393 - val_loss: 0.5385 - val_accuracy: 0.7637\n","Epoch 24/40\n","6244/6244 [==============================] - 289s 46ms/step - loss: 0.5303 - accuracy: 0.8392 - val_loss: 0.5555 - val_accuracy: 0.7541\n","Epoch 25/40\n","6244/6244 [==============================] - 286s 46ms/step - loss: 0.5319 - accuracy: 0.8376 - val_loss: 0.5629 - val_accuracy: 0.7524\n","Epoch 26/40\n","6244/6244 [==============================] - 285s 46ms/step - loss: 0.5301 - accuracy: 0.8388 - val_loss: 0.5473 - val_accuracy: 0.7608\n","Epoch 27/40\n","6244/6244 [==============================] - 292s 47ms/step - loss: 0.5307 - accuracy: 0.8392 - val_loss: 0.5410 - val_accuracy: 0.7617\n","Epoch 28/40\n","6244/6244 [==============================] - 287s 46ms/step - loss: 0.5305 - accuracy: 0.8392 - val_loss: 0.5459 - val_accuracy: 0.7592\n","Epoch 29/40\n","6244/6244 [==============================] - 290s 47ms/step - loss: 0.5297 - accuracy: 0.8394 - val_loss: 0.5540 - val_accuracy: 0.7554\n","Epoch 30/40\n","6244/6244 [==============================] - 301s 48ms/step - loss: 0.5312 - accuracy: 0.8400 - val_loss: 0.5475 - val_accuracy: 0.7595\n","Epoch 31/40\n","6244/6244 [==============================] - 301s 48ms/step - loss: 0.5304 - accuracy: 0.8392 - val_loss: 0.5322 - val_accuracy: 0.7674\n","Epoch 32/40\n","6244/6244 [==============================] - 286s 46ms/step - loss: 0.5311 - accuracy: 0.8383 - val_loss: 0.5508 - val_accuracy: 0.7584\n","Epoch 33/40\n","6244/6244 [==============================] - 286s 46ms/step - loss: 0.5305 - accuracy: 0.8391 - val_loss: 0.5432 - val_accuracy: 0.7633\n","Epoch 34/40\n","6244/6244 [==============================] - 289s 46ms/step - loss: 0.5302 - accuracy: 0.8395 - val_loss: 0.5550 - val_accuracy: 0.7551\n","Epoch 35/40\n","6244/6244 [==============================] - 289s 46ms/step - loss: 0.5310 - accuracy: 0.8383 - val_loss: 0.5300 - val_accuracy: 0.7702\n","Epoch 36/40\n","6244/6244 [==============================] - 289s 46ms/step - loss: 0.5306 - accuracy: 0.8390 - val_loss: 0.5653 - val_accuracy: 0.7490\n","Epoch 37/40\n","6244/6244 [==============================] - 289s 46ms/step - loss: 0.5288 - accuracy: 0.8391 - val_loss: 0.5533 - val_accuracy: 0.7568\n","Epoch 38/40\n","6244/6244 [==============================] - 286s 46ms/step - loss: 0.5299 - accuracy: 0.8390 - val_loss: 0.5515 - val_accuracy: 0.7609\n","Epoch 39/40\n","6244/6244 [==============================] - 288s 46ms/step - loss: 0.5298 - accuracy: 0.8396 - val_loss: 0.5502 - val_accuracy: 0.7576\n","Epoch 40/40\n","6244/6244 [==============================] - 286s 46ms/step - loss: 0.5296 - accuracy: 0.8393 - val_loss: 0.5457 - val_accuracy: 0.7632\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Model training with the Adam optimizer\n","\"\"\"\n","# Build the network with some parameters as parameters like\n","# the input size of network and number of categories\n","model = CancerNet.build(width=48, height=48, depth=3, class_num=2)\n","# Build the Adam optimizer\n","opt = Adam(learning_rate=INIT_LR)\n","# Compile the whole model\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","# Create the callback of the dynamically-changed learning rate\n","# factor: how much the learning rate decreased at each epoch\n","# min_lr: the bottom limit of LR\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n","# Set the checkpoint to save the best model\n","checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', monitor='val_loss', verbose=0,\n","               save_best_only=True, mode='auto')\n","\n","# reference link:https://keras.io/examples/structured_data/imbalanced_classification/\n","class_weight={0: class_weight[0], 1: class_weight[1]}\n","# Training the model with the function fit_generator\n","training_records = model.fit(\n","    train_gen,\n","    steps_per_epoch = train_len//BATCH_SIZE,\n","    validation_data = val_gen,\n","    validation_steps = valid_len//BATCH_SIZE,\n","    class_weight = class_weight,\n","    epochs = EPOCHS_NUM,\n","    callbacks = [reduce_lr, checkpoint])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_A12Eou4CKuv","executionInfo":{"status":"ok","timestamp":1643297659892,"user_tz":-120,"elapsed":18124329,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"4bfcccad-621c-4349-cc1a-97bf1ece3a07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","6244/6244 [==============================] - 461s 72ms/step - loss: 0.6173 - accuracy: 0.8099 - val_loss: 0.4409 - val_accuracy: 0.8381 - lr: 0.0100\n","Epoch 2/40\n","6244/6244 [==============================] - 459s 74ms/step - loss: 0.5639 - accuracy: 0.8296 - val_loss: 0.4936 - val_accuracy: 0.8437 - lr: 0.0100\n","Epoch 3/40\n","6244/6244 [==============================] - 456s 73ms/step - loss: 0.5187 - accuracy: 0.8443 - val_loss: 0.3640 - val_accuracy: 0.8455 - lr: 0.0020\n","Epoch 4/40\n","6244/6244 [==============================] - 436s 70ms/step - loss: 0.5094 - accuracy: 0.8468 - val_loss: 0.3102 - val_accuracy: 0.8682 - lr: 0.0020\n","Epoch 5/40\n","6244/6244 [==============================] - 440s 70ms/step - loss: 0.5076 - accuracy: 0.8478 - val_loss: 0.3430 - val_accuracy: 0.8565 - lr: 0.0020\n","Epoch 6/40\n","6244/6244 [==============================] - 445s 71ms/step - loss: 0.4962 - accuracy: 0.8517 - val_loss: 0.3667 - val_accuracy: 0.8471 - lr: 4.0000e-04\n","Epoch 7/40\n","6244/6244 [==============================] - 447s 72ms/step - loss: 0.4930 - accuracy: 0.8524 - val_loss: 0.3981 - val_accuracy: 0.8285 - lr: 1.0000e-04\n","Epoch 8/40\n","6244/6244 [==============================] - 446s 71ms/step - loss: 0.4931 - accuracy: 0.8526 - val_loss: 0.3840 - val_accuracy: 0.8344 - lr: 1.0000e-04\n","Epoch 9/40\n","6244/6244 [==============================] - 443s 71ms/step - loss: 0.4916 - accuracy: 0.8529 - val_loss: 0.3857 - val_accuracy: 0.8363 - lr: 1.0000e-04\n","Epoch 10/40\n","6244/6244 [==============================] - 443s 71ms/step - loss: 0.4893 - accuracy: 0.8524 - val_loss: 0.3912 - val_accuracy: 0.8280 - lr: 1.0000e-04\n","Epoch 11/40\n","6244/6244 [==============================] - 433s 69ms/step - loss: 0.4904 - accuracy: 0.8528 - val_loss: 0.3822 - val_accuracy: 0.8347 - lr: 1.0000e-04\n","Epoch 12/40\n","6244/6244 [==============================] - 431s 69ms/step - loss: 0.4907 - accuracy: 0.8525 - val_loss: 0.3875 - val_accuracy: 0.8362 - lr: 1.0000e-04\n","Epoch 13/40\n","6244/6244 [==============================] - 435s 70ms/step - loss: 0.4882 - accuracy: 0.8534 - val_loss: 0.3918 - val_accuracy: 0.8330 - lr: 1.0000e-04\n","Epoch 14/40\n","6244/6244 [==============================] - 436s 70ms/step - loss: 0.4900 - accuracy: 0.8527 - val_loss: 0.3834 - val_accuracy: 0.8350 - lr: 1.0000e-04\n","Epoch 15/40\n","6244/6244 [==============================] - 428s 69ms/step - loss: 0.4870 - accuracy: 0.8538 - val_loss: 0.3864 - val_accuracy: 0.8378 - lr: 1.0000e-04\n","Epoch 16/40\n","6244/6244 [==============================] - 433s 69ms/step - loss: 0.4885 - accuracy: 0.8536 - val_loss: 0.3854 - val_accuracy: 0.8388 - lr: 1.0000e-04\n","Epoch 17/40\n","6244/6244 [==============================] - 440s 70ms/step - loss: 0.4872 - accuracy: 0.8534 - val_loss: 0.3765 - val_accuracy: 0.8385 - lr: 1.0000e-04\n","Epoch 18/40\n","6244/6244 [==============================] - 441s 71ms/step - loss: 0.4865 - accuracy: 0.8537 - val_loss: 0.3963 - val_accuracy: 0.8379 - lr: 1.0000e-04\n","Epoch 19/40\n","6244/6244 [==============================] - 438s 70ms/step - loss: 0.4876 - accuracy: 0.8531 - val_loss: 0.3710 - val_accuracy: 0.8376 - lr: 1.0000e-04\n","Epoch 20/40\n","6244/6244 [==============================] - 441s 71ms/step - loss: 0.4875 - accuracy: 0.8540 - val_loss: 0.3932 - val_accuracy: 0.8399 - lr: 1.0000e-04\n","Epoch 21/40\n","6244/6244 [==============================] - 433s 69ms/step - loss: 0.4874 - accuracy: 0.8535 - val_loss: 0.4354 - val_accuracy: 0.8343 - lr: 1.0000e-04\n","Epoch 22/40\n","6244/6244 [==============================] - 451s 72ms/step - loss: 0.4860 - accuracy: 0.8533 - val_loss: 0.3594 - val_accuracy: 0.8492 - lr: 1.0000e-04\n","Epoch 23/40\n","6244/6244 [==============================] - 441s 71ms/step - loss: 0.4854 - accuracy: 0.8549 - val_loss: 0.4159 - val_accuracy: 0.8321 - lr: 1.0000e-04\n","Epoch 24/40\n","6244/6244 [==============================] - 440s 70ms/step - loss: 0.4851 - accuracy: 0.8553 - val_loss: 0.4591 - val_accuracy: 0.8316 - lr: 1.0000e-04\n","Epoch 25/40\n","6244/6244 [==============================] - 445s 71ms/step - loss: 0.4853 - accuracy: 0.8538 - val_loss: 0.4389 - val_accuracy: 0.8340 - lr: 1.0000e-04\n","Epoch 26/40\n","6244/6244 [==============================] - 441s 71ms/step - loss: 0.4840 - accuracy: 0.8545 - val_loss: 0.5364 - val_accuracy: 0.8315 - lr: 1.0000e-04\n","Epoch 27/40\n","6244/6244 [==============================] - 441s 71ms/step - loss: 0.4846 - accuracy: 0.8552 - val_loss: 0.4477 - val_accuracy: 0.8316 - lr: 1.0000e-04\n","Epoch 28/40\n","6244/6244 [==============================] - 442s 71ms/step - loss: 0.4836 - accuracy: 0.8547 - val_loss: 0.5647 - val_accuracy: 0.8387 - lr: 1.0000e-04\n","Epoch 29/40\n","6244/6244 [==============================] - 439s 70ms/step - loss: 0.4832 - accuracy: 0.8543 - val_loss: 0.5548 - val_accuracy: 0.8288 - lr: 1.0000e-04\n","Epoch 30/40\n","6244/6244 [==============================] - 434s 70ms/step - loss: 0.4835 - accuracy: 0.8537 - val_loss: 0.5824 - val_accuracy: 0.8381 - lr: 1.0000e-04\n","Epoch 31/40\n","6244/6244 [==============================] - 450s 72ms/step - loss: 0.4841 - accuracy: 0.8541 - val_loss: 0.4556 - val_accuracy: 0.8297 - lr: 1.0000e-04\n","Epoch 32/40\n","6244/6244 [==============================] - 447s 72ms/step - loss: 0.4840 - accuracy: 0.8549 - val_loss: 0.4840 - val_accuracy: 0.8333 - lr: 1.0000e-04\n","Epoch 33/40\n","6244/6244 [==============================] - 444s 71ms/step - loss: 0.4842 - accuracy: 0.8545 - val_loss: 0.4728 - val_accuracy: 0.8338 - lr: 1.0000e-04\n","Epoch 34/40\n","6244/6244 [==============================] - 441s 71ms/step - loss: 0.4829 - accuracy: 0.8546 - val_loss: 0.4625 - val_accuracy: 0.8329 - lr: 1.0000e-04\n","Epoch 35/40\n","6244/6244 [==============================] - 444s 71ms/step - loss: 0.4829 - accuracy: 0.8545 - val_loss: 0.4421 - val_accuracy: 0.8341 - lr: 1.0000e-04\n","Epoch 36/40\n","6244/6244 [==============================] - 449s 72ms/step - loss: 0.4820 - accuracy: 0.8553 - val_loss: 0.4170 - val_accuracy: 0.8343 - lr: 1.0000e-04\n","Epoch 37/40\n","6244/6244 [==============================] - 448s 72ms/step - loss: 0.4822 - accuracy: 0.8544 - val_loss: 0.4562 - val_accuracy: 0.8357 - lr: 1.0000e-04\n","Epoch 38/40\n","6244/6244 [==============================] - 439s 70ms/step - loss: 0.4819 - accuracy: 0.8561 - val_loss: 0.8618 - val_accuracy: 0.8309 - lr: 1.0000e-04\n","Epoch 39/40\n","6244/6244 [==============================] - 452s 72ms/step - loss: 0.4820 - accuracy: 0.8549 - val_loss: 0.4844 - val_accuracy: 0.8346 - lr: 1.0000e-04\n","Epoch 40/40\n","6244/6244 [==============================] - 428s 69ms/step - loss: 0.4826 - accuracy: 0.8543 - val_loss: 0.4742 - val_accuracy: 0.8415 - lr: 1.0000e-04\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Model evaluation\n","part1: drawing the accuracy change curve during the training process\n","The convergence of the model trained with Adagrad otimizer is stable, but has the slow convergence speed.\n","The convergence of the model trained with Adam otimizer is not stable, but has the high convergence speed.\n","\"\"\"\n","N = EPOCHS_NUM\n","M = training_records\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), M.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), M.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), M.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), M.history[\"val_accuracy\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy on the IDC Dataset\")\n","plt.xlabel(\"Epoch No.\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.show()\n","plt.savefig('plot.png')"],"metadata":{"id":"ymv4kahUIJR-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Model evaluation\n","part2: take the model with the highest validation accuracy to make predictions: a model saved at 4th epoch with the training accuracy\n","accuracy 0.8468 and validation accuracy 0.8682.  \n","\"\"\"\n","model = load_model('model-004.h5')\n","print(\"Now evaluating the model\")\n","test_gen.reset() # the required operation before the evaluation\n","# make predictions to the iterator test_gen\n","pred_indices = model.predict_generator(test_gen, steps=(test_len//BATCH_SIZE)+1)\n","# prediction results are one-hot encoding, return the index of the element\n","# with the largest number per row\n","pred_indices = np.argmax(pred_indices, axis=1)\n","\n","# sklearn classification_report\n","# usage: sklearn.metrics.classification_report(y_true, y_pred, *, \n","# labels=None, target_names=None, sample_weight=None, digits=2, \n","# output_dict=False, zero_division='warn')\n","print(classification_report(test_gen.classes, pred_indices, \n","              target_names=test_gen.class_indices.keys()))\n","# calculate the confusion matrix\n","cm_matrix = confusion_matrix(test_gen.classes, pred_indices)\n","# calculate the number of total training samples\n","total = sum(sum(cm_matrix))\n","# calculate the accuracy, specificity, sentivitiy, and f1 score\n","accuracy = (cm_matrix[0, 0] + cm_matrix[1, 1])/total\n","specificity = cm_matrix[1, 1]/(cm_matrix[1, 0] + cm_matrix[1, 1])\n","sensitivity = cm_matrix[0, 0]/(cm_matrix[0, 0] + cm_matrix[0, 1]) # recall\n","recall = sensitivity\n","precision = cm_matrix[0, 0]/(cm_matrix[0, 0] + cm_matrix[1, 0])\n","f1_score = (2 * precision * recall)/(precision + recall)\n","# print all results of evaluation indices\n","print(cm_matrix)\n","print(f'Accuracy: {accuracy}')\n","print(f'Specificity: {specificity}')\n","print(f'Sensitivity: {sensitivity}')\n","print(f'F1 score: {f1_score}')"],"metadata":{"id":"-4yudv8RvhWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643369536332,"user_tz":-120,"elapsed":29144,"user":{"displayName":"TUO YANG","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14621909467358785593"}},"outputId":"99e780c8-0d45-42f7-a435-659d2035ec0a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Now evaluating the model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      0.93      0.91     39736\n","           1       0.80      0.68      0.73     15769\n","\n","    accuracy                           0.86     55505\n","   macro avg       0.84      0.81      0.82     55505\n","weighted avg       0.86      0.86      0.86     55505\n","\n","[[37012  2724]\n"," [ 5034 10735]]\n","Accuracy: 0.8602288082154761\n","Specificity: 0.6807660599911218\n","Sensitivity: 0.931447553855446\n","F1 score: 0.9051380499376391\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"kDXRBG-RcMYo"},"execution_count":null,"outputs":[]}]}